\documentclass[a4paper,9pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage[margin=1in, paperheight=400mm]{geometry}
\usepackage{fontspec}

\graphicspath{{img/}}

\title{\vspace*{-1in}Summary report for Machine Learning}

\author{Magdalena Pakuła \& Jakub Pawlak}

\date{}

\begin{document}

\maketitle


\section{KNN approach}
\subsection{Overview}

Our KNN classifier is parametrized by the number of neighbors $k$, and the distance function, $d$.
When given a list of movies the user has watched, and a new movie to predict, it calculates the distance between the new movie and each of the watched movies, and selects $k$ closest to the new movie.
After that, ratings of all the selected movies is performed to produce a single predicted rating for the new movie.
The aggregation we chose is the review that occurs most often (bincount followed by argmax).

\paragraph{Movie features}
For each movie, we extracted the following data from TMDB:
budget, popularity, release year, revenue, runtime, vote average, vote count, genres, cast, director.

One of the first remarks made was that there are different kinds of features, which we distinguished as follows.
First, there are \emph{scalar} features, which are just numbers, like the revenue or duration.
Second, there are categorical features, which have some set of possible values, and take one of them,
and lastly, there are multi-valued features, like cast of the movie.

Categorical and multi-valued features could be one-hot encoded, however this would create several issues.
Firstly, one-hot encoding requires that all of the possible values are present in the training set.
If there was a new movie that had an actor or director that was not present in the training, there would not be a way to encode it.
Secondly, the number of output dimensions of one-hot encoding depends on the characteristics of input data, which would impact the relative influence of certain features on the total distance (similarity).

For these reasons, we decided to compute distances (similarities) separately, and use their linear combination for a final result, used for comparing movies. Such approach would also allow us to chose diffent measures, based on the data characteristics.

\paragraph{Data Preprocessing}
To prepare the features for similarity calculations, the scalar features were scaled to a range $[0,1]$, using min-max scaler fitted to the entirety of the training data.

\paragraph{Similarity Measures}
The similarity between two movies was computed as a weighted sum of component wise similarities,
using different metrics for various feature types:

\begin{itemize}
	\item \textbf{Scalar features:} The euclidean similarity metric was primarily used for scalar features. Other metrics, such as manhattan and cosine distances, were evaluated for comparison.
	\item \textbf{Categorical Features:} Jaccard similarity measured the overlap between sets (e.g., genres, cast).
	\item \textbf{Rating Similarity:} We also noticed that we can utilize the training dataset and treat movies as similar, if they have similar ratings by different users. Such rating vectors would however contain many empty entries, therefore we used the cosine distance, because it handles such cases well.
\end{itemize}

The resulting of calculating these distances, was a 5-element vector (one for scalar similarity, 3 for genres, cast and directors, and 1 for rating-based similarity).
The resulting measure would be a dot product of this vector with a 5-element weight vector.
The weight vector could be scaled such that all of its elements add up to 1, however this is not a requirement, as it does not distrupt the ordering.
This weight vector would be another hyperparameter of the model, which was experimented with until we found values that worked best.

\subsection{Results}
The results demonstrated that incorporating user rating similarity significantly improved the accuracy of predictions, as it captures shared user preferences. Additionally, the combination of scalar, categorical, and rating-based similarities provided a balanced metric, ensuring that no single feature type dominated the predictions.

The K-Nearest Neighbour (KNN) model was trained on a subset of the dataset, using a combination of numerical and categorical features. The optimal number of neighbors (\texttt{n\_neighbors}) was determined through grid search, achieving the highest accuracy with \( k = 5 \).

Moreover, the testing shown that the model's performance did not vary very much when experimenting with the values of the weight vector, which may suggest that the user's rating does not depend as much on the movie characteristics, as on the user characteristics (e.g.~users give ``generally good'' ratings, regardless of the movie).
Alternatively, it could suggest a problem with the dataset, which is not statistically representative of the whole movie spectrum. As training data, we use the movies that a given user has watched and rated. However, it is a reasonable assumption that the user will generally tend to watch movies that they like. Therefore, both the training set, and by extension, validation set, are representative of movies that the user tends to watch.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Metric}     & \textbf{Training Data} & \textbf{Validation Data} \\ \hline
		Correct predictions & 0.56                   & 0.37                     \\ \hline
		One-off predictions & 0.21                   & 0.31                     \\ \hline
	\end{tabular}
	\caption{Metrics on the Training and Validation Datasets}
	\label{tab:metrics}
\end{table}

\clearpage
\section{Decision Tree Approach}
\subsection{Overview}

The Decision Tree operates by recursively splitting the dataset based on feature values to create
a hierarchical model, aiming to maximize the homogeneity of the resulting subsets.

\paragraph{Splitting criterion}
The key component of a decision tree is what we call a ``choice''. Because of the distinction of different types of movie features, we describe 3 types of choices:
\begin{itemize}
	\item Scalar choice: For numerical features, tests if a value is smaller or larger than a given threshold,
	\item Categorical choice: For categorical features, tests if it is equal to some value from a set of possible values
	\item Set choice: For multi-valued features, tests if the feature set contains some value
\end{itemize}

Each choice is parametrized by one value. At each step when we create the choice node in the tree, we gather a set of all possible choices, which is union over all features, and all values of that feature, of the appropriate choice type parametrized by that value.

\begin{equation}
	\text{Choices} = \bigcup\limits_{f \in F} \bigcup\limits_{v \in M[f]} c_f(v)
\end{equation}
where $F$ is the set of all features, $M[f]$ is the set of all values of feature $f$ in the set of movies $M$,
and $c_f(v)$ is the appropriate choice type for feature $f$, parametrized by value $v$.

From this set of all possible choices, we find the best one using Gini impurity of the resulting sets.
Each choice splits the set of movies $M$ into sets $M^+$ and $M^-$ of movies that respectively passed and failed the test.

The Gini impurity represents the probabilities of miscategorizing items from each class, and is defined as follws:
\begin{equation}
	I_G(p) = 1 - \sum_{i=1}^{k} p_i^2,
\end{equation}

This measure is calculated for each set $M^+$ and $M^-$, and their weighted average is the score of a split.
The best split is one that minimizes this measure.
After choosing the best set, the child nodes are constructed recursively on sets $M^+$ and $M^-$.

\paragraph{Stopping criteria}
There are 2 cases when recursive training is stopped:
\begin{itemize}
	\item Maximum depth is reached
	\item The training set has only one rating --- in which case splitting further would make no sense.
\end{itemize}


An example of a decision tree induced for a specific user is shown in Figure~\ref{fig:decision_tree}, illustrating how user-specific features were leveraged to make predictions.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\textwidth]{tree}
	\caption{Visualization of an induced decision tree for a single user.}
	\label{fig:decision_tree}
\end{figure}

\subsection{Results}
The performance metrics indicated:

\begin{table}[ht]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Metric}     & \textbf{Training Data} & \textbf{Validation Data} \\ \hline
		Correct predictions & 0.67                   & 0.40                     \\ \hline
		One-off predictions & 0.16                   & 0.29                     \\ \hline
	\end{tabular}
	\caption{Metrics on the Training and Validation Datasets}
	\label{tab:decision_tree_results}
\end{table}

Decision trees demonstrated capability to fit specific user preferences, resulting in scores higher than KNN.
They were able to identify patterns such as disliked actors or favorite genres. Moreover, their underlying representation is easy to reason about.

The drawback of decision trees is that they are prone to overfitting. Even small changes in the training data can completely change the preferred choice of the tree. This makes this model very sensitive to noise or outliers in the training dataset, leading to poor generalization capabilities.

The behavior of the tree is controlled by its depth, which is a hyperparameter of the model.
If the tree is too shallow, it may fail to capture the user's preferences, but if it's too high, the tree might become very sensitive to outliers, which can be seen in the example image, where in the deep nodes, the tree creates braches for specific movies. We chose the depth of our tree by gradually increasing it until the accuracy on the validation dataset stopped increasing.

\clearpage
\section{Random Forest Approach}
\subsection{Overview}
The Random Forest approach is an ensemble learning method that combines multiple decision trees
to enhance predictive performance and reduce overfitting.
By aggregating the predictions of several individual trees, Random Forest mitigates the limitations of
single decision trees, such as sensitivity to noise and high variance.
The ensemble's diversity was assured through two key mechanisms: the random selection of feature
subsets for each tree and the use of bootstrapped training datasets, ensuring that each tree received
a unique perspective on the data.

The Random Forest model was implemented with the
following key components:

\begin{itemize}
	\item \textbf{Random Feature Selection}:
	      A custom class, \texttt{\_RandomFeatureSelector}, was created to randomly select a subset of features for each tree. This ensured that individual trees focused on different attributes of the data, reducing the correlation between trees in the ensemble. For example, features like \texttt{budget}, \texttt{genres}, and \texttt{popularity} were considered, while identifiers such as \texttt{title} were excluded to enhance generalization.
	\item \textbf{Bootstrapping}:
	      To further ensure diversity, bootstrapped training datasets were generated by sampling movies with replacement from the original dataset. Each bootstrapped sample included selected features and their corresponding ratings.
	\item \textbf{Binary Decision Trees}:
	      Each decision tree in the Random Forest was built using binary splits, where node conditions were satisfied or not. The maximum tree depth was restricted to 5 to balance computational efficiency and model expressiveness. The trees were trained to optimize the Gini impurity at each split, ensuring effective feature selection during induction.
	\item \textbf{Aggregation of Predictions}:
	      The Random Forest model combined the predictions of all trees in the ensemble using an averaging function. This aggregation reduced the variance of individual predictions and improved the robustness of the final recommendation scores.
\end{itemize}


These principles collectively ensure that the Random Forest model reduces overfitting while maintaining high predictive accuracy.

Since decision trees are interpretable models, it is possible to visualize the decision-making process of individual trees within the Random Forest. Figure~\ref{fig:random_forest_trees} illustrates sample decision trees from the ensemble for a single user. Each tree captures a unique hierarchical structure based on feature thresholds, contributing to the ensemble's diversity.
The first tree reflects preferences based on actors and release year, the second one --- popularity, revenue, and budget, and the last one --- genres, budget and directors.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.25\textwidth]{forest/tree_0.png}
	\includegraphics[width=0.25\textwidth]{forest/tree_9.png}
	\includegraphics[width=0.25\textwidth]{forest/tree_4.png}
	\caption{Sample decision trees from the Random Forest ensemble for a single user. Each tree reflects a unique subset of features and bootstrapped training data.}
	\label{fig:random_forest_trees}
\end{figure}

\subsection{Results}
The performance metrics indicated:

\begin{table}[ht]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Metric}     & \textbf{Training Data} & \textbf{Validation Data} \\ \hline
		Correct predictions & 0.49                   & 0.36                     \\ \hline
		One-off predictions & 0.37                   & 0.40                     \\ \hline
	\end{tabular}
	\caption{Metrics on the Training and Validation Datasets}
	\label{tab:random_forest_results}
\end{table}

Surprisingly the random forest approach did not outperform the decision tree as expected.
We can notice however that the random forest did not have as much drop from training set to validation set performance.
This suggests that is is overfitting less, as expected.

Although it has lower correct prediciton rates, it outperforms the decision tree in the one-off errors, suggesting that it did sacrifice some exact prediction capabilities for more consistent near-accurate predictions. This may suggest that overall the random forest approach is more robust than a single decision tree.

Another reason might be the wrong choice of hyperparameters. Decision tree has only one, but random forest in addition to the depth of the trees has the number of trees, the way of choosing features and bootstrapping the data, and finally, the aggregation function. This increased number of hyperparameters makes it much more difficult to perform an exhaustive grid search. This is additionally amplified by the fact that random forest is much more computationally intensive that a single decision tree by at least the factor of number of trees.

\clearpage
\section{Person Similarity Approach}
\subsection{Overview}
The Person Similarity approach was employed to predict user ratings for movies by leveraging the similarity
between users based on their historical ratings.
The primary goal was to identify users with similar movie preferences and use their ratings to estimate
the target user's missing ratings.
This approach exclusively considers user evaluations, disregarding movie-specific features, and focuses
on users who have rated the same movies as the target user.

The methodology for similarity measure involved the combination of \textbf{Pearson correlation} and \textbf{Cosine similarity}:

\begin{itemize}
	\item \textbf{Pearson Correlation:} Captures the linear relationship between two users' ratings, centering around their respective mean ratings to account for rating biases.
	      \begin{equation}
		      \operatorname{sim}_\text{pearson}(x,y) = \frac{\sum_{i} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i} (x_i - \bar{x})^2 \sum_{i} (y_i - \bar{y})^2}},
	      \end{equation}
	      where \( x_i \) and \( y_i \) are ratings given by two users for the same movies, and \( \bar{x} \), \( \bar{y} \) are their respective mean ratings.

	\item \textbf{Cosine Similarity:} Evaluates the angular similarity between two users' rating vectors, ignoring the magnitude of their ratings:
	      \begin{equation}
		      \operatorname{sim}_\text{cosine}(x,y) = \frac{\sum_{i} x_i y_i}{\sqrt{\sum_{i} x_i^2} \sqrt{\sum_{i} y_i^2}}.
	      \end{equation}
\end{itemize}


A weighting factor, $w$, was used to blend the two metrics:
\begin{equation}
	\operatorname{sim}(x,y) = w \cdot \operatorname{sim}_\text{pearson}(x,y) + (1 - w) \cdot \operatorname{sim}_\text{cosine}(x,y) .
\end{equation}
This approach allowed flexibility in prioritizing either metric based on the validation performance.

To mitigate the influence of users with very few overlapping ratings, a damping factor was applied, scaling down the
similarity score when the number of shared ratings was low.

For efficiency, computed similarity scores were cached in a dictionary, avoiding redundant calculations when the same user
pairs were compared multiple times.

To predict the target user's rating for a specific movie, only users who had rated the target movie and shared common
rated movies with the target user were considered for prediction.

The predicted rating, \( \hat{r}_{u,m} \), was calculated using a weighted average of ratings from similar users:
\begin{equation}
	\hat{r}_{u,m} = \frac{\sum_{v \in N(m)} sim(u,v) \cdot r_{v,m}}{\sum_{v \in N(m)} |sim(u,v)|},
\end{equation}
where \( \hat{r}_{u,m} \) is the predicted rating for user \( u \) on movie \( m \), \( sim(u,v) \) is the similarity score between user \( u \) and user \( v \), and \( r_{v,m} \) is the rating given by user \( v \) to movie \( m \). \( N(m) \) denotes the set of users who have rated movie \( m \).

In cases where insufficient overlapping ratings were available, predictions defaulted to the target user's global average rating for better generalization.

\subsection{Results}
The performance metrics indicated:

\begin{table}[ht]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Metric}     & \textbf{Training Data} & \textbf{Validation Data} \\ \hline
		Correct predictions & 0.34                   & 0.32                     \\ \hline
		One-off predictions & 0.49                   & 0.46                     \\ \hline
	\end{tabular}
	\caption{Metrics on the Training and Validation Datasets}
	\label{tab:person_similarity_results}
\end{table}

The Person Similarity approach demonstrated a strong ability to predict user ratings,
particularly in scenarios where users had rated a substantial number of common movies.
However, its performance was more limited when the overlap of rated movies was small, as the approach relies heavily
on the availability of common ratings for comparison.

In conclusion, while the Person Similarity approach provides an effective method for movie rating prediction based on user
similarities, its performance can be impacted by the sparsity of the rating matrix.
Nonetheless, it remains a valuable technique for collaborative filtering in recommender systems.

\clearpage
\section{Collaborative Filtering Approach}


\subsection{Overview}

This machine learning model utilizes \textbf{collaborative filtering} with matrix factorization to predict user ratings for movies. The goal is to uncover features for both users and movies, which are learned through optimization. By modeling user preferences and movie characteristics in a shared feature space, the system can predict ratings for unseen user-movie pairs.

\paragraph{Key Components of the Model}

\begin{itemize}
	\item \textbf{User and Movie Features}:
	      Each user is represented by a vector of parameters \( \mathbf{p}_u \) consisting of a bias term \( p_{u,0} \) and feature weights \( p_{u,1}, p_{u,2}, \dots, p_{u,N} \), where \( N \) is the number of features.
	      Similarly, each movie is represented by a vector of feature weights \( \mathbf{x}_m \), corresponding to the same features.

	\item \textbf{Prediction}:
	      The predicted rating for a user \( u \) and movie \( m \) is calculated as:
	      \[
		      \hat{y}_{m,u} = p_{u,0} + \sum_{n=1}^N p_{u,n} x_{m,n},
	      \]
	      where \( \hat{y}_{m,u} \) represents the predicted rating, and the summation captures the interaction between user and movie features.

	\item \textbf{Loss Function}:
	      The model is trained by minimizing the squared error between the predicted ratings \( \hat{y}_{m,u} \) and the actual ratings \( y_{m,u} \) for all observed user-movie pairs:
	      \[
		      Q = \frac{1}{2} \sum_{\text{observed } (m, u)} \left( \hat{y}_{m,u} - y_{m,u} \right)^2 + \frac{\lambda}{2} \left( \|\mathbf{P}\|^2 + \|\mathbf{X}\|^2 \right),
	      \]
	      where \( \lambda \) is a regularization parameter to prevent overfitting, and \( \|\cdot\|^2 \) denotes the sum of squared elements in the user parameter matrix \( \mathbf{P} \) and movie feature matrix \( \mathbf{X} \).

	\item \textbf{Optimization}:
	      Gradients for user parameters \( \mathbf{P} \) and movie features \( \mathbf{X} \) are derived as follows:
	      \begin{align*}
		      \frac{\partial Q}{\partial p_{u,0}} & = \sum_{\text{observed } (m,u)} (\hat{y}_{m,u} - y_{m,u}),                           \\
		      \frac{\partial Q}{\partial p_{u,n}} & = \sum_{\text{observed } (m,u)} (\hat{y}_{m,u} - y_{m,u}) x_{m,n} + \lambda p_{u,n}, \\
		      \frac{\partial Q}{\partial x_{m,n}} & = \sum_{\text{observed } (m,u)} (\hat{y}_{m,u} - y_{m,u}) p_{u,n} + \lambda x_{m,n}.
	      \end{align*}

	      These derivatives can be expressed in matrix multiplication form for efficient computation:
	      \begin{align*}
		      \nabla_{\mathbf{P}} & = (\mathbf{\hat{Y}} - \mathbf{Y})^\top \mathbf{X} + \lambda \mathbf{P}, \\
		      \nabla_{\mathbf{X}} & = (\mathbf{\hat{Y}} - \mathbf{Y}) \mathbf{P} + \lambda \mathbf{X},
	      \end{align*}
	      where matrix $\mathbf{X}$ is augmented with a column of ones to account for the lack of bias term.
	\item \textbf{Initialization}:
	      User parameters and movie features are initialized with small random values. The model iteratively refines these parameters to reduce the loss function.
\end{itemize}

\subsection{Results}
The training phase achieved a stable loss value after multiple iterations.
The following metrics were used to evaluate performance:

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Metric}     & \textbf{Training Data} & \textbf{Validation Data} \\ \hline
		Correct Predictions & 0.54                   & 0.38                     \\ \hline
		One-Off Predictions & 0.39                   & 0.38                     \\ \hline
	\end{tabular}
	\caption{Performance Metrics on Training and Validation Datasets}
	\label{tab:predictions_metrics}
\end{table}

The model was validated on a separate dataset to assess its ability to generalize.
Cross-validation techniques (e.g., \(k\)-fold cross-validation with \(k=5\)) were employed to mitigate variance due to data partitioning.
These results demonstrate the effectiveness of the collaborative filtering approach, with a balance between
fitting the training data and generalizing to unseen data.

\clearpage
\section{Summary and Comparison}
In this section, we present the comparative results of different machine
learning approaches for predicting user ratings on movies.
Each method was evaluated on training, validation, and testing datasets.
The results are presented in Table ~\ref{tab:comparison_accuracy} and Table
~\ref{tab:comparison_oneoff}.


\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Method} & \textbf{Training Accuracy} & \textbf{Validation Accuracy} & \textbf{Testing Accuracy} \\ \hline
        KNN & 0.56 & 0.37 & 0.36 \\ \hline
        Decision Tree & 0.67 & 0.40 & 0.39 \\ \hline
        Random Forest & 0.49 & 0.36 & 0.37 \\ \hline
        Person Similarity & 0.34 & 0.32 & 0.27 \\ \hline
        Collaborative Filtering & 0.54 & 0.38 & 0.41 \\ \hline
    \end{tabular}
    \caption{Comparison of Training, Validation, and Testing Accuracies}
    \label{tab:comparison_accuracy}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Method} & \textbf{Training One-Off} & \textbf{Validation One-Off} & \textbf{Testing One-Off} \\ \hline
        KNN & 0.21 & 0.31 & 0.30 \\ \hline
        Decision Tree & 0.16 & 0.29 & 0.31 \\ \hline
        Random Forest & 0.37 & 0.40 & 0.39 \\ \hline
        Person Similarity & 0.49 & 0.46 & 0.44 \\ \hline
        Collaborative Filtering & 0.39 & 0.38 & 0.37 \\ \hline
    \end{tabular}
    \caption{Comparison of One-Off Predictions for Training, Validation, and Testing}
    \label{tab:comparison_oneoff}
\end{table}

\paragraph{KNN Approach:}
The KNN model demonstrated moderate performance by utilizing features of the movies and other user ratings on selected movies.
However, its performance on testing data was lower than on training and validation sets.
This discrepancy suggests potential overfitting and issues with dataset representativeness, as users often rate only movies they enjoy, leading to biases in the data.
Despite its simplicity, KNN’s reliance on dense data limits its scalability and effectiveness in sparse datasets.

\paragraph{Decision Tree Approach:}
Decision Trees showed a structured approach to capturing user preferences, resulting in better performance than the KNN approach.
However, their sensitivity to noise in the training data and tendency to overfit resulted in a significant decline in performance on unseen data.
The notable drop in accuracy from training to testing data underscores the importance of careful hyperparameter tuning to improve generalization.
While Decision Trees provided valuable insights into user behavior through their structured decision-making process, their usability diminishes in sparse datasets where overlapping user-movie interactions are limited.

\paragraph{Random Forest Approach:}
As expected, Random Forests addressed the overfitting limitations of single Decision Trees, offering a more balanced performance across training, validation, and testing data.
While the exact prediction rate was slightly lower than Decision Trees, the model's robustness was evident in its ability to generalize and achieve the second highest one-off prediction rate.
This improved performance can be attributed to the ensemble's ability to reduce noise and overfitting, making it a reliable option for capturing broader user preferences.
However, challenges in hyperparameter tuning may have constrained its full potential.
These results suggest Random Forests may be more effective in scenarios requiring interpretability and resilience to noisy data.

\paragraph{Person Similarity Approach:}
The Person Similarity method was effective in dense data scenarios where users shared a significant number of common ratings.
However, its reliance on overlapping ratings made it vulnerable to sparse datasets, resulting in lower overall accuracy.
Despite these challenges, this method achieved the highest one-off prediction rate, which underscores its usefulness in collaborative filtering scenarios, especially for datasets with strong user-item interactions.

\paragraph{Collaborative Filtering Approach:}
Collaborative Filtering emerged as the most effective method, achieving the highest testing accuracy and strong one-off prediction rates.
Its balanced performance across training, validation, and testing data highlights its ability to leverage user-movie interaction patterns effectively.
This approach excelled in generalization, making it particularly suitable for scenarios with sparse datasets, where capturing nuanced user preferences is essential.
This performance underscores the importance of understanding user preferences and interactions in movie rating predictions.

\paragraph{Conclusions:}
Overall, each method showcased distinct strengths and limitations.
KNN and Collaborative Filtering excel in generalization, while Decision Trees and Random Forests provide deeper insights into specific user preferences, but struggled with overfitting.
The Person Similarity approach is particularly effective in scenarios with high overlap between user ratings, but underperformed with sparse data.
Collaborative Filtering stood out as the most effective technique, indicating its potential for robust movie rating predictions.

The consistently high one-off prediction rates across methods suggest that, while exact matches are challenging, the models can approximate user preferences effectively.
These results highlight the importance of addressing dataset biases to ensure reliable generalization in predictive modeling.


\end{document}
